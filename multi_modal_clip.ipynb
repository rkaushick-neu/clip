{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a CLIP Inspired Multi-Modal Model\n",
    "\n",
    "Submitted by: Rishabh Kaushick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment Setup (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet torch torchvision transformers datasets matplotlib tqdm pillow ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPTokenizer, DistilBertTokenizer, DistilBertModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "Is CUDA available: False\n",
      "CUDA is not available. Running on CPU.\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    " \n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.2f} MB\")\n",
    "        print(f\"Memory Reserved: {torch.cuda.memory_reserved(i) / 1024**2:.2f} MB\")\n",
    " \n",
    "else:\n",
    "    print(\"CUDA is not available. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Selection and Exploration (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, I have decided to use the Flickr30k dataset. The reasons for this are as follows:\n",
    "1.\tThe Flickr30k dataset contains much lesser images as compared to the COCO dataset. This will be easier to train due to the GPU limitations.\n",
    "2.\tThe Flickr30k dataset contains high quality natural language captions. The COCO dataset has complex captions which could introduce more variability making the training process more difficult.\n",
    "3.\tFlickr30k dataset is suited for retrieval-based tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Flickr30k dataset from Hugging Face\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\", split=\"test[:10000]\")\n",
    "\n",
    "# looks like the main train split is called test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have considered the first 10k records so that it is easier to train the data with limited time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to save the dataset in /data/flickr10k\n",
    "dataset.save_to_disk(\"./data/flickr10k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset from Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"./data/flickr10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'caption', 'sentids', 'split', 'img_id', 'filename'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetInfo(description='', citation='', homepage='', license='', features={'image': Image(mode=None, decode=True, id=None), 'caption': [Value(dtype='string', id=None)], 'sentids': [Value(dtype='string', id=None)], 'split': Value(dtype='string', id=None), 'img_id': Value(dtype='string', id=None), 'filename': Value(dtype='string', id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='flickr30k', config_name='TEST', version=1.1.0, splits={'test': SplitInfo(name='test', num_bytes=4326502343, num_examples=31014, shard_lengths=[3800, 3700, 3700, 3800, 3600, 3400, 3300, 3600, 2114], dataset_name='flickr30k')}, download_checksums={'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0000.parquet': {'num_bytes': 505667437, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0001.parquet': {'num_bytes': 501896374, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0002.parquet': {'num_bytes': 505834156, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0003.parquet': {'num_bytes': 511840780, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0004.parquet': {'num_bytes': 503648349, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0005.parquet': {'num_bytes': 495305542, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0006.parquet': {'num_bytes': 495475995, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0007.parquet': {'num_bytes': 496924754, 'checksum': None}, 'hf://datasets/nlphuji/flickr30k@cd91f9a00273ce2e1584511cba8c10b917c488a3/TEST/test/0008.parquet': {'num_bytes': 289359289, 'checksum': None}}, download_size=4305952676, post_processing_size=None, dataset_size=4326502343, size_in_bytes=8632455019)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': Image(mode=None, decode=True, id=None),\n",
       " 'caption': [Value(dtype='string', id=None)],\n",
       " 'sentids': [Value(dtype='string', id=None)],\n",
       " 'split': Value(dtype='string', id=None),\n",
       " 'img_id': Value(dtype='string', id=None),\n",
       " 'filename': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image     10000 non-null  object\n",
      " 1   caption   10000 non-null  object\n",
      " 2   sentids   10000 non-null  object\n",
      " 3   split     10000 non-null  object\n",
      " 4   img_id    10000 non-null  object\n",
      " 5   filename  10000 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 468.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split (Train, Val, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    train\n",
       "1    train\n",
       "2    train\n",
       "3    train\n",
       "4    train\n",
       "5    train\n",
       "6    train\n",
       "7    train\n",
       "8    train\n",
       "9    train\n",
       "Name: split, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['split'][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we have all three in the 10k subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    9375\n",
       "test      324\n",
       "val       301\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_value_counts = dataset_df['split'].value_counts()\n",
    "split_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have all 3 as shown above. Now let's check how many of each set is present in the 10k records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Count: 9375 (93.75%)\n",
      "Validation Set Count: 301 (3.01%)\n",
      "Testing Set Count: 324 (3.24%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Set Count: {split_value_counts['train']} ({(split_value_counts['train']/10000)*100:.2f}%)\")\n",
    "print(f\"Validation Set Count: {split_value_counts['val']} ({(split_value_counts['val']/10000)*100:.2f}%)\")\n",
    "print(f\"Testing Set Count: {split_value_counts['test']} ({(split_value_counts['test']/10000)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the train set is more than 90%, and the test and validation sets are much smaller in comparison. \n",
    "\n",
    "In this project, I will use a custom train-val-test split:\n",
    "- 80% training\n",
    "- 10% validation \n",
    "- 10% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns('split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first creating 80% train, 20% test\n",
    "train_test_data = dataset.train_test_split(test_size=0.2, seed=23)\n",
    "\n",
    "# out of the 20% test taking half of it as train and test again for validation\n",
    "val_test_data = train_test_data['test'].train_test_split(test_size=0.5, seed=23)\n",
    "\n",
    "train_dataset = train_test_data['train']\n",
    "val_dataset = val_test_data['train']\n",
    "test_dataset = val_test_data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.array([train_dataset.num_rows, val_dataset.num_rows, test_dataset.num_rows])\n",
    "labels = ['Training Data', 'Validation Data', 'Test Data']\n",
    "plt.pie(y, labels=labels, explode=[0.1,0.1,0.1])\n",
    "plt.title(\"Data Split\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the training set\n",
    "sample = train_dataset[0]\n",
    "image = sample[\"image\"] \n",
    "caption = sample[\"caption\"]\n",
    "\n",
    "# Display Image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Sample Image & Caption from Flickr30k')\n",
    "plt.text(0.5, -0.05, caption[0], ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the training set\n",
    "sample = train_dataset[0]\n",
    "image = sample[\"image\"]\n",
    "caption = sample[\"caption\"]\n",
    "\n",
    "# Display Image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "# Update to show all captions\n",
    "plt.title('Sample Image & Caption from Flickr30k')\n",
    "plt.text(0.5, -0.05, \"\\n\".join(caption), ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the training set\n",
    "sample = train_dataset[10]\n",
    "image = sample[\"image\"]\n",
    "caption = sample[\"caption\"]\n",
    "\n",
    "# Display Image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "# Update to show all captions\n",
    "plt.title('Sample Image & Caption from Flickr30k')\n",
    "plt.text(0.5, -0.05, \"\\n\".join(caption), ha='center', va='top', transform=plt.gca().transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that for our training process, we will only require the image and the captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resizing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = train_dataset[0]['image']\n",
    "image_width, image_height = image_data.size\n",
    "print(f\"Width: {image_width}, Height: {image_height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = train_dataset[1020]['image']\n",
    "image_width, image_height = image_data.size\n",
    "print(f\"Width: {image_width}, Height: {image_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as though each image has a different width and height, which is not favorable for the model. Let's first convert them all into a standard width and height of 128 x 128. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = train_dataset[0]['image']\n",
    "print(\"Image before resizing\")\n",
    "image_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image after resizing:\")\n",
    "image_data.resize((128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now resizing all the images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(example):\n",
    "    example['resized_image'] = example['image'].resize((128, 128))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use to(device) below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing the images in train and validation sets\n",
    "resized_train_dataset = train_dataset.map(resize_image)\n",
    "resized_val_dataset = val_dataset.map(resize_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_dataset['image'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_dataset['resized_image'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that all images have been resized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Pixel Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code is inefficient, need to find better ways to find the min & max pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = np.min(resized_train_dataset['resized_image'])\n",
    "max_value = np.max(resized_train_dataset['resized_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Image pixel values range from {min_value} to {max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly performing normalization on all the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(example):\n",
    "    example['normalized_image'] = np.array(example['resized_image'])/255.0\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use .to(device) below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = resized_train_dataset.map(normalize_image)\n",
    "preprocessed_val_dataset = resized_val_dataset.map(normalize_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is taking too long to load\n",
    "# resized_dataset['image'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is taking too long to load\n",
    "# resized_dataset['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = preprocessed_train_dataset.select(range(10))  # Access the first 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset[0]['image'][0][0:10]\n",
    "subset[0]['normalized_image'][0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can see that the image now has values between 0 and 1 since we normalized it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Captions\n",
    "I've decided to use the DistilBERT tokenizer for the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly creating a method to tokenize the entire captions column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(example):\n",
    "    example['tokenized_caption'] = tokenizer(example['caption'], max_length=77, padding=\"max_length\", truncation=True)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_dataset = preprocessed_train_dataset.map(tokenize_captions)\n",
    "preprocessed_val_dataset = preprocessed_val_dataset.map(tokenize_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = preprocessed_train_dataset.select(range(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[0]['tokenized_caption'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[0]['tokenized_caption']['attention_mask'][0][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we tokenize the captions, the DistilBERT Tokenizer creates 'attention masks' and 'input ids' as we can see above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Design (30%) \n",
    "\n",
    "![CLIP model design](./references/CLIP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Encoder\n",
    "\n",
    "- The ResNet-18 model will be used as the backbone CNN model for feature extraction.\n",
    "- The final layer of the ResNet18 will be removed\n",
    "- A projection layer will be added at the end of the ResNet18 model to map data into a 512 dimension shared embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        resnet = models.resnet18(weights=True)\n",
    "        # Remove the final classification layer\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # Add a projection layer to map to the joint embedding space\n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from the ResNet backbone\n",
    "        features = self.backbone(x)\n",
    "        # Flatten the features\n",
    "        features = features.view(features.size(0), -1)\n",
    "        # Project to embedding space\n",
    "        embeddings = self.projection(features)\n",
    "        # Normalize embeddings to have unit norm\n",
    "        normalized_embeddings = self.norm(embeddings)\n",
    "        return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Encoder\n",
    "\n",
    "- DistilBERT will be used as the backbone model for processing text.\n",
    "- Most parameters except the last layer will be frozen to reduce training time. Since the DistilBERT architecture consists of 6 layers (0 to 5), we will freeze layers 0 to 4 and allow the layer 5 to updated \n",
    "during training.\n",
    "- Will takes the [CLS] token embedding as the sentence representation.\n",
    "- Will project this to the same 512-dimensional embedding space as the image encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=512):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        # Load pretrained DistilBERT model\n",
    "        self.transformer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        # Freeze most of the transformer layers (0 to 4) to reduce training time and prevent overfitting\n",
    "        for name, param in self.transformer.named_parameters():\n",
    "            if 'layer.5' not in name:  # DistilBERT has 6 layers (0-5), only train the last one (layer 5)\n",
    "                param.requires_grad = False\n",
    "        # DistilBERT outputs 768-dimensional features- so going from 768 to 512 (same embedding size as the image):\n",
    "        self.projection = nn.Linear(768, 512)\n",
    "        # Normalization layer\n",
    "        self.norm = nn.LayerNorm(512)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get the [CLS] token embedding from DistilBERT\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token as the sentence representation\n",
    "        # For DistilBERT, this is the first token of the last hidden state\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        # Project to the joint embedding space\n",
    "        embeddings = self.projection(cls_embedding)\n",
    "        # Normalize embeddings to have unit norm\n",
    "        normalized_embeddings = self.norm(embeddings)\n",
    "        return normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model for Shared Embedding Space\n",
    "\n",
    "- Combines both encoders into a single model.\n",
    "- Includes a method to compute contrastive loss between image and text embeddings.\n",
    "- Uses temperature scaling to control the sharpness of similarity distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, embed_dim=512, temperature=0.07):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        # Create the image encoder\n",
    "        self.image_encoder = ImageEncoder(embed_dim=embed_dim)\n",
    "        # Create the text encoder\n",
    "        self.text_encoder = TextEncoder(embed_dim=embed_dim)\n",
    "        # Temperature parameter for the loss calculation\n",
    "        self.temperature = temperature\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Encode images and text\n",
    "        image_embeddings = self.image_encoder(images)  # Shape: [batch_size, embed_dim]\n",
    "        input_ids = input_ids[:, 0, :]  # Shape: [batch_size, 77]\n",
    "        attention_mask = attention_mask[:, 0, :]  # Shape: [batch_size, 77]\n",
    "\n",
    "        text_embeddings = self.text_encoder(input_ids, attention_mask)  # Shape: [batch_size, embed_dim]\n",
    "        # Normalize embeddings (optional if already done in individual encoders)\n",
    "        image_embeddings = F.normalize(image_embeddings, p=2, dim=1)\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
    "        return image_embeddings, text_embeddings\n",
    "    \n",
    "    def compute_contrastive_loss(self, image_embeddings, text_embeddings):\n",
    "        # Compute cosine similarity matrix\n",
    "        # Shape: [batch_size, batch_size]\n",
    "        logits = torch.matmul(image_embeddings, text_embeddings.t()) / self.temperature\n",
    "        # Create labels: diagonal elements should match (same index = positive pair)\n",
    "        labels = torch.arange(logits.shape[0], device=logits.device)\n",
    "        # Calculate loss for image-to-text direction\n",
    "        image_loss = F.cross_entropy(logits, labels)\n",
    "        # Calculate loss for text-to-image direction\n",
    "        text_loss = F.cross_entropy(logits.t(), labels)\n",
    "        # The total loss is the average of the two directional losses\n",
    "        total_loss = (image_loss + text_loss) / 2\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Process (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# initialize hyper parameters\n",
    "embed_dim = 512\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# tokenizer\n",
    "# tokenizer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset \n",
    "\n",
    "While creating the dataset, we need to create a transforms pipeline to preprocess the data the same way we processed it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        # normalizing vales are based on the standard normalization parameters from the original ResNet-18 documentation\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageTextDataset\n",
    "This will be a custom Dataset that contains the images and the corresponding captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, images, captions, tokenizer, transform=None, max_length=77):\n",
    "        self.images = images\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load and transform image\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        caption = self.captions[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            return_tensors='pt',\n",
    "            max_length=77, # the original CLIP paper uses max_length as 77\n",
    "            # 77 tokens provides enough context to capture meaningful text descriptions while remaining computationally efficient\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'caption': caption  # Keep original caption for reference\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # since we have already preprocessed the data - we don't need any transforms or tokenizers\n",
    "\n",
    "# class ImageTextDataset(Dataset):\n",
    "#     def __init__(self, images, captions, tokenized_caption, max_length=77):\n",
    "#         self.images = images\n",
    "#         self.captions = captions\n",
    "#         self.tokenized_captions = tokenized_caption\n",
    "#         self.max_length = max_length\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Load the image at idx location\n",
    "#         image = self.images[idx]\n",
    "\n",
    "#         # Get the text at idx location\n",
    "#         caption = self.captions[idx]\n",
    "\n",
    "#         # Get the tokenized caption at idx location\n",
    "#         encoding = tokenize_captions[idx]\n",
    "        \n",
    "#         return {\n",
    "#             'image': image,\n",
    "#             'input_ids': encoding['input_ids'].squeeze(),\n",
    "#             'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "#             'caption': caption  # Keep original caption for reference\n",
    "#         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code does all the preprocessing in one step and creates it as a ImageTextDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when I was taking the data without preprocessing it first\n",
    "\n",
    "train_dataset = ImageTextDataset(\n",
    "    train_dataset['image'], \n",
    "    train_dataset['caption'], \n",
    "    tokenizer, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataset = ImageTextDataset(\n",
    "    val_dataset['image'], \n",
    "    val_dataset['caption'], \n",
    "    tokenizer, \n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the data (train & val) into a dataset after full preprocessing:\n",
    "\n",
    "# train_pytorch_dataset = ImageTextDataset(\n",
    "#     preprocessed_train_dataset['image'],\n",
    "#     preprocessed_train_dataset['caption'],\n",
    "#     preprocessed_train_dataset['tokenized_caption']\n",
    "# )\n",
    "\n",
    "# val_pytorch_dataset = ImageTextDataset(\n",
    "#     preprocessed_val_dataset['image'],\n",
    "#     preprocessed_val_dataset['caption'],\n",
    "#     preprocessed_val_dataset['tokenized_caption']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Now the train_dataset & val_dataset have been processed and are objects of type 'ImageTextDataset' -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ImageTextDataset"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(train_pytorch_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "    \n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     train_pytorch_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "    \n",
    "# val_loader = DataLoader(\n",
    "#     val_pytorch_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=4,\n",
    "#     pin_memory=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clip_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, device='cuda'):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler (optional)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Training and validation metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # input_ids = input_ids[:, 0, :]  # Shape: [64, 77]\n",
    "            # attention_mask = attention_mask[:, 0, :]  # Shape: [64, 77]\n",
    "\n",
    "            # print(\"Input IDs shape:\", input_ids.shape) # Input IDs shape: torch.Size([64, 77])\n",
    "            # print(\"Attention mask shape:\", attention_mask.shape) # Attention mask shape: torch.Size([64, 77])\n",
    "            \n",
    "            # Forward pass\n",
    "            image_embeddings, text_embeddings = model(images, input_ids, attention_mask)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = model.compute_contrastive_loss(image_embeddings, text_embeddings)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate average training loss for this epoch\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "            for batch in pbar:\n",
    "                # Move data to device\n",
    "                images = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "                # input_ids = input_ids[:, 0, :]  # Shape: [64, 77]\n",
    "                # attention_mask = attention_mask[:, 0, :]  # Shape: [64, 77]\n",
    "\n",
    "                \n",
    "                # Forward pass\n",
    "                image_embeddings, text_embeddings = model(images, input_ids, attention_mask)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = model.compute_contrastive_loss(image_embeddings, text_embeddings)\n",
    "                \n",
    "                # Update metrics\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        # Calculate average validation loss for this epoch\n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint (optional)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, f'clip_checkpoint_epoch_{epoch+1}.pt')\n",
    "    \n",
    "    # Return training history\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|| 125/125 [05:31<00:00,  2.66s/it, loss=2.19]\n",
      "Epoch 1/10 [Val]: 100%|| 16/16 [00:22<00:00,  1.41s/it, loss=2.11]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 2.9920, Val Loss: 2.3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]:  57%|    | 71/125 [03:07<02:22,  2.65s/it, loss=1.56]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m CLIPModel(embed_dim\u001b[38;5;241m=\u001b[39membed_dim)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_clip_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 39\u001b[0m, in \u001b[0;36mtrain_clip_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# input_ids = input_ids[:, 0, :]  # Shape: [64, 77]\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# attention_mask = attention_mask[:, 0, :]  # Shape: [64, 77]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m image_embeddings, text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_contrastive_loss(image_embeddings, text_embeddings)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[0;34m(self, images, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, input_ids, attention_mask):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Encode images and text\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, embed_dim]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Shape: [batch_size, 77]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Shape: [batch_size, 77]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Extract features from the ResNet backbone\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Flatten the features\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mview(features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torchvision/models/resnet.py:103\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m     identity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m    102\u001b[0m out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m--> 103\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/csye7230/lib/python3.10/site-packages/torch/nn/functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = CLIPModel(embed_dim=embed_dim)\n",
    "\n",
    "# Train model\n",
    "train_losses, val_losses = train_clip_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=learning_rate,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training your model\n",
    "torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation and Results (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csye7230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
